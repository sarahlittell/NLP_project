{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> Import Libraries</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.text import Text\n",
    "\n",
    "plt.style.use('seaborn-v0_8-pastel')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> Data Ingestion</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"TwitterHateSpeech.csv\", usecols = ['label', 'tweet'])\n",
    "data_stats={'Observations':df.shape[0],'Features':df.shape[1],'File_size':df.size,\n",
    "            'Columns':df.columns,'Data_types':df.dtypes,'Null_vals':df.isnull().sum()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> Data Understanding</font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; Predictor and Target Attributes</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[:, 1:]\n",
    "label = df.iloc[:, 0:1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; Pie Chart</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hateSpeech=df[df['label']==1].shape[0] #w/o .shape[0] is just all hate speech, shape0 gets cols, 1 gets rows\n",
    "normSpeech=df[df['label']==0].shape[0]\n",
    "\n",
    "typesSpeech=[hateSpeech,normSpeech]\n",
    "labels=['Hate Speech','Non-hate speech']\n",
    "plt.pie(typesSpeech,explode=[0,.1],labels=labels,autopct='%1.1f%%',startangle=-50)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; Word Clouds</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Hate Speech Word Cloud</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_speech = df[df['label'] == 1]   \n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "for val in hate_speech.tweet:\n",
    "    val = str(val)\n",
    "    tokens = val.split()\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='black',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 12).generate(comment_words)\n",
    "                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Normal Speech Word Cloud</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normSpeech = df[df['label'] == 0]   \n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "for val in normSpeech.tweet:\n",
    "    val = str(val)\n",
    "    tokens = val.split()\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='black',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 12).generate(comment_words)\n",
    "                 \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> Data Cleaning</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; Text Cleaning</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"Makes all input text lowercase, cleans of @users, \n",
    "    removes punctuation and special characters\"\"\"\n",
    "    newtext=''\n",
    "    newtext=text.lower()\n",
    "    newtext=re.sub(r'(@[A-Za-z0-9]+)',\"\",text)\n",
    "    newtext=text.translate(str.maketrans('','',string.punctuation))\n",
    "    newtext=\" \".join(e for e in text.split() if e.isalnum())\n",
    "    return newtext\n",
    "df['tweet']=df['tweet'].apply(clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; Pre-Processing</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Tokenizing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "def tokenize(text):\n",
    "    text=word_tokenize(text)\n",
    "    return text\n",
    "df['tweet']=df['tweet'].apply(tokenize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Removing StopWords</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')\n",
    "def StopWords(text): #\n",
    "    \"\"\"\"removes stop words from text\"\"\"\n",
    "    return ' '.join([word for word in text if word not in (stop)])\n",
    "\n",
    "df['tweet']=df['tweet'].apply(StopWords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Lemmatization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return [lemmatizer.lemmatize(token) for token in text]\n",
    "\n",
    "# df['tweet']=df['tweet'].apply(lemmatize) #breaks stuff, fix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> Create ML Model</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; Featrue Extraction</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(max_features=5000)\n",
    "corpus=df['tweet']\n",
    "tfidf_matrix=vectorizer.fit_transform(corpus)\n",
    "text=tfidf_matrix.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = #FFE6E6> &nbsp; &nbsp; &nbsp; &nbsp; Split Dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels=df.iloc[:,0]\n",
    "x_train, x_test, y_train, y_test=train_test_split(text,labels,test_size=.3,random_state=0)\n",
    "\n",
    "print('Training data:',x_train.shape)\n",
    "print('Testing data:',x_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
